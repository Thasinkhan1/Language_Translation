{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:24.520107Z",
     "iopub.status.busy": "2025-07-11T16:32:24.519768Z",
     "iopub.status.idle": "2025-07-11T16:32:24.523716Z",
     "shell.execute_reply": "2025-07-11T16:32:24.522850Z",
     "shell.execute_reply.started": "2025-07-11T16:32:24.520058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:24.525011Z",
     "iopub.status.busy": "2025-07-11T16:32:24.524798Z",
     "iopub.status.idle": "2025-07-11T16:32:24.541794Z",
     "shell.execute_reply": "2025-07-11T16:32:24.540950Z",
     "shell.execute_reply.started": "2025-07-11T16:32:24.524987Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use GPU\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  \n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:28.129009Z",
     "iopub.status.busy": "2025-07-11T16:32:28.128692Z",
     "iopub.status.idle": "2025-07-11T16:32:28.133240Z",
     "shell.execute_reply": "2025-07-11T16:32:28.132449Z",
     "shell.execute_reply.started": "2025-07-11T16:32:28.128981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from indicnlp import common\n",
    "from indicnlp.tokenize import indic_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:28.135357Z",
     "iopub.status.busy": "2025-07-11T16:32:28.135139Z",
     "iopub.status.idle": "2025-07-11T16:32:28.149341Z",
     "shell.execute_reply": "2025-07-11T16:32:28.148657Z",
     "shell.execute_reply.started": "2025-07-11T16:32:28.135339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:28.150506Z",
     "iopub.status.busy": "2025-07-11T16:32:28.150320Z",
     "iopub.status.idle": "2025-07-11T16:32:28.705271Z",
     "shell.execute_reply": "2025-07-11T16:32:28.704534Z",
     "shell.execute_reply.started": "2025-07-11T16:32:28.150487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"google-T5/T5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:28.706996Z",
     "iopub.status.busy": "2025-07-11T16:32:28.706701Z",
     "iopub.status.idle": "2025-07-11T16:32:28.755056Z",
     "shell.execute_reply": "2025-07-11T16:32:28.754412Z",
     "shell.execute_reply.started": "2025-07-11T16:32:28.706968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/kaggle/input/sentence-pair-english-to-hindi/Sentence pairs in English-Hindi - 2025-02-13.tsv\",sep=\"\\t\",header=None,names=[\"SrcSentenceID\",\"SrcSentence\",\"DstSentenceID\",\"DstSentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:28.756045Z",
     "iopub.status.busy": "2025-07-11T16:32:28.755784Z",
     "iopub.status.idle": "2025-07-11T16:32:28.764392Z",
     "shell.execute_reply": "2025-07-11T16:32:28.763616Z",
     "shell.execute_reply.started": "2025-07-11T16:32:28.756024Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SrcSentenceID</th>\n",
       "      <th>SrcSentence</th>\n",
       "      <th>DstSentenceID</th>\n",
       "      <th>DstSentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1282</td>\n",
       "      <td>Muiriel is 20 now.</td>\n",
       "      <td>485968</td>\n",
       "      <td>म्यूरियल अब बीस साल की हो गई है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1282</td>\n",
       "      <td>Muiriel is 20 now.</td>\n",
       "      <td>2060319</td>\n",
       "      <td>म्यूरियल अब बीस साल की है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1294</td>\n",
       "      <td>Education in this world disappoints me.</td>\n",
       "      <td>485564</td>\n",
       "      <td>मैं इस दुनिया में शिक्षा पर बहुत निराश हूँ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1302</td>\n",
       "      <td>That won't happen.</td>\n",
       "      <td>2060320</td>\n",
       "      <td>वैसा नहीं होगा।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1308</td>\n",
       "      <td>I miss you.</td>\n",
       "      <td>2060321</td>\n",
       "      <td>मुझें तुम्हारी याद आ रही है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SrcSentenceID                              SrcSentence  DstSentenceID  \\\n",
       "0           1282                       Muiriel is 20 now.         485968   \n",
       "1           1282                       Muiriel is 20 now.        2060319   \n",
       "2           1294  Education in this world disappoints me.         485564   \n",
       "3           1302                       That won't happen.        2060320   \n",
       "4           1308                              I miss you.        2060321   \n",
       "\n",
       "                                   DstSentence  \n",
       "0             म्यूरियल अब बीस साल की हो गई है।  \n",
       "1                   म्यूरियल अब बीस साल की है।  \n",
       "2  मैं इस दुनिया में शिक्षा पर बहुत निराश हूँ।  \n",
       "3                              वैसा नहीं होगा।  \n",
       "4                 मुझें तुम्हारी याद आ रही है।  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:28.765354Z",
     "iopub.status.busy": "2025-07-11T16:32:28.765151Z",
     "iopub.status.idle": "2025-07-11T16:32:28.779622Z",
     "shell.execute_reply": "2025-07-11T16:32:28.778989Z",
     "shell.execute_reply.started": "2025-07-11T16:32:28.765336Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13182, 4)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:28.782023Z",
     "iopub.status.busy": "2025-07-11T16:32:28.781837Z",
     "iopub.status.idle": "2025-07-11T16:32:28.926955Z",
     "shell.execute_reply": "2025-07-11T16:32:28.926333Z",
     "shell.execute_reply.started": "2025-07-11T16:32:28.782007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data[\"DstSentence\"]=data[\"DstSentence\"].apply(lambda x: indic_tokenize.trivial_tokenize(x,lang=\"hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:28.928466Z",
     "iopub.status.busy": "2025-07-11T16:32:28.928185Z",
     "iopub.status.idle": "2025-07-11T16:32:28.935420Z",
     "shell.execute_reply": "2025-07-11T16:32:28.934631Z",
     "shell.execute_reply.started": "2025-07-11T16:32:28.928442Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [म्यूरियल, अब, बीस, साल, की, हो, गई, है, ।]\n",
       "1                      [म्यूरियल, अब, बीस, साल, की, है, ।]\n",
       "2        [मैं, इस, दुनिया, में, शिक्षा, पर, बहुत, निराश...\n",
       "3                                    [वैसा, नहीं, होगा, ।]\n",
       "4                    [मुझें, तुम्हारी, याद, आ, रही, है, ।]\n",
       "                               ...                        \n",
       "13177            [क्या, आपके, पास, सब्ज़ी, -, चावल, है, ?]\n",
       "13178            [क्या, आपके, पास, सब्ज़ी, -, चावल, है, ?]\n",
       "13179            [क्या, आपके, पास, सब्ज़ी, -, चावल, है, ?]\n",
       "13180            [क्या, आपके, पास, सब्ज़ी, -, चावल, है, ?]\n",
       "13181              [मुझे, यह, साइकिल, अब, भी, पसंद, है, ।]\n",
       "Name: DstSentence, Length: 13182, dtype: object"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"DstSentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:28.936323Z",
     "iopub.status.busy": "2025-07-11T16:32:28.936115Z",
     "iopub.status.idle": "2025-07-11T16:32:28.954331Z",
     "shell.execute_reply": "2025-07-11T16:32:28.953684Z",
     "shell.execute_reply.started": "2025-07-11T16:32:28.936306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Nd=max(list(data[\"DstSentence\"].apply(len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:28.955307Z",
     "iopub.status.busy": "2025-07-11T16:32:28.955109Z",
     "iopub.status.idle": "2025-07-11T16:32:28.970553Z",
     "shell.execute_reply": "2025-07-11T16:32:28.969930Z",
     "shell.execute_reply.started": "2025-07-11T16:32:28.955289Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:28.971815Z",
     "iopub.status.busy": "2025-07-11T16:32:28.971540Z",
     "iopub.status.idle": "2025-07-11T16:32:29.699592Z",
     "shell.execute_reply": "2025-07-11T16:32:29.698887Z",
     "shell.execute_reply.started": "2025-07-11T16:32:28.971786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data[\"SrcSentence\"]=data[\"SrcSentence\"].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:29.700571Z",
     "iopub.status.busy": "2025-07-11T16:32:29.700355Z",
     "iopub.status.idle": "2025-07-11T16:32:29.707661Z",
     "shell.execute_reply": "2025-07-11T16:32:29.706953Z",
     "shell.execute_reply.started": "2025-07-11T16:32:29.700552Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        [▁Mu, i, riel, ▁is, ▁20, ▁now, .]\n",
       "1                        [▁Mu, i, riel, ▁is, ▁20, ▁now, .]\n",
       "2        [▁Education, ▁in, ▁this, ▁world, ▁disappoint, ...\n",
       "3                          [▁That, ▁won, ', t, ▁happen, .]\n",
       "4                                     [▁I, ▁miss, ▁you, .]\n",
       "                               ...                        \n",
       "13177    [▁Do, ▁you, ▁have, ▁some, ▁curry, ▁and, ▁some,...\n",
       "13178    [▁Do, ▁you, ▁have, ▁curry, ▁and, ▁some, ▁rice, ?]\n",
       "13179    [▁Do, ▁you, ▁have, ▁any, ▁curry, ▁with, ▁rice, ?]\n",
       "13180          [▁Do, ▁you, ▁have, ▁curry, ▁with, ▁rice, ?]\n",
       "13181              [▁I, ▁still, ▁love, ▁this, ▁bicycle, .]\n",
       "Name: SrcSentence, Length: 13182, dtype: object"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"SrcSentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:29.708567Z",
     "iopub.status.busy": "2025-07-11T16:32:29.708370Z",
     "iopub.status.idle": "2025-07-11T16:32:29.727515Z",
     "shell.execute_reply": "2025-07-11T16:32:29.726852Z",
     "shell.execute_reply.started": "2025-07-11T16:32:29.708549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seq_len=max(list(data[\"SrcSentence\"].apply(len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:29.728470Z",
     "iopub.status.busy": "2025-07-11T16:32:29.728210Z",
     "iopub.status.idle": "2025-07-11T16:32:29.745148Z",
     "shell.execute_reply": "2025-07-11T16:32:29.744505Z",
     "shell.execute_reply.started": "2025-07-11T16:32:29.728443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:29.745992Z",
     "iopub.status.busy": "2025-07-11T16:32:29.745809Z",
     "iopub.status.idle": "2025-07-11T16:32:29.782463Z",
     "shell.execute_reply": "2025-07-11T16:32:29.781659Z",
     "shell.execute_reply.started": "2025-07-11T16:32:29.745976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Vs = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:29.783662Z",
     "iopub.status.busy": "2025-07-11T16:32:29.783369Z",
     "iopub.status.idle": "2025-07-11T16:32:29.798724Z",
     "shell.execute_reply": "2025-07-11T16:32:29.797961Z",
     "shell.execute_reply.started": "2025-07-11T16:32:29.783630Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32100"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:29.799645Z",
     "iopub.status.busy": "2025-07-11T16:32:29.799427Z",
     "iopub.status.idle": "2025-07-11T16:32:29.825890Z",
     "shell.execute_reply": "2025-07-11T16:32:29.825124Z",
     "shell.execute_reply.started": "2025-07-11T16:32:29.799616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Vd = set()\n",
    "for tokenized_hindi_sentence in data['DstSentence']:\n",
    "    Vd.update(tokenized_hindi_sentence)\n",
    "    \n",
    "hindi_vocab = dict()\n",
    "\n",
    "for idx, token in enumerate(Vd):\n",
    "    hindi_vocab[token] = idx + 4\n",
    "\n",
    "hindi_vocab[\"<PAD>\"] = 0\n",
    "hindi_vocab[\"<SOS>\"] = 1\n",
    "hindi_vocab[\"<EOS>\"] = 2\n",
    "hindi_vocab[\"<UNK>\"] = 3\n",
    "\n",
    "Vd = hindi_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:29.826911Z",
     "iopub.status.busy": "2025-07-11T16:32:29.826715Z",
     "iopub.status.idle": "2025-07-11T16:32:29.848982Z",
     "shell.execute_reply": "2025-07-11T16:32:29.848165Z",
     "shell.execute_reply.started": "2025-07-11T16:32:29.826894Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eng_vocab = dict()\n",
    "for idx, token in enumerate(Vs):\n",
    "    eng_vocab[token] = idx + 4  # reserve 0–3 for special tokens\n",
    "\n",
    "eng_vocab[\"<PAD>\"] = 0\n",
    "eng_vocab[\"<SOS>\"] = 1\n",
    "eng_vocab[\"<EOS>\"] = 2\n",
    "eng_vocab[\"<UNK>\"] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:29.849902Z",
     "iopub.status.busy": "2025-07-11T16:32:29.849692Z",
     "iopub.status.idle": "2025-07-11T16:32:29.864866Z",
     "shell.execute_reply": "2025-07-11T16:32:29.864045Z",
     "shell.execute_reply.started": "2025-07-11T16:32:29.849885Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7073"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Vd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:29.865793Z",
     "iopub.status.busy": "2025-07-11T16:32:29.865579Z",
     "iopub.status.idle": "2025-07-11T16:32:29.880273Z",
     "shell.execute_reply": "2025-07-11T16:32:29.879563Z",
     "shell.execute_reply.started": "2025-07-11T16:32:29.865775Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32104"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:29.881240Z",
     "iopub.status.busy": "2025-07-11T16:32:29.880960Z",
     "iopub.status.idle": "2025-07-11T16:32:29.893465Z",
     "shell.execute_reply": "2025-07-11T16:32:29.892806Z",
     "shell.execute_reply.started": "2025-07-11T16:32:29.881209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#jis bhasa me translate krna hai us bhasa me teeno Token chahiye SOS EOS PAD\n",
    "#jis se translate kr rha hai usme jarurat nhi hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:29.898350Z",
     "iopub.status.busy": "2025-07-11T16:32:29.898153Z",
     "iopub.status.idle": "2025-07-11T16:32:29.915536Z",
     "shell.execute_reply": "2025-07-11T16:32:29.914727Z",
     "shell.execute_reply.started": "2025-07-11T16:32:29.898329Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SrcSentenceID</th>\n",
       "      <th>SrcSentence</th>\n",
       "      <th>DstSentenceID</th>\n",
       "      <th>DstSentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1282</td>\n",
       "      <td>[▁Mu, i, riel, ▁is, ▁20, ▁now, .]</td>\n",
       "      <td>485968</td>\n",
       "      <td>[म्यूरियल, अब, बीस, साल, की, हो, गई, है, ।]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1282</td>\n",
       "      <td>[▁Mu, i, riel, ▁is, ▁20, ▁now, .]</td>\n",
       "      <td>2060319</td>\n",
       "      <td>[म्यूरियल, अब, बीस, साल, की, है, ।]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1294</td>\n",
       "      <td>[▁Education, ▁in, ▁this, ▁world, ▁disappoint, ...</td>\n",
       "      <td>485564</td>\n",
       "      <td>[मैं, इस, दुनिया, में, शिक्षा, पर, बहुत, निराश...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1302</td>\n",
       "      <td>[▁That, ▁won, ', t, ▁happen, .]</td>\n",
       "      <td>2060320</td>\n",
       "      <td>[वैसा, नहीं, होगा, ।]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1308</td>\n",
       "      <td>[▁I, ▁miss, ▁you, .]</td>\n",
       "      <td>2060321</td>\n",
       "      <td>[मुझें, तुम्हारी, याद, आ, रही, है, ।]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SrcSentenceID                                        SrcSentence  \\\n",
       "0           1282                  [▁Mu, i, riel, ▁is, ▁20, ▁now, .]   \n",
       "1           1282                  [▁Mu, i, riel, ▁is, ▁20, ▁now, .]   \n",
       "2           1294  [▁Education, ▁in, ▁this, ▁world, ▁disappoint, ...   \n",
       "3           1302                    [▁That, ▁won, ', t, ▁happen, .]   \n",
       "4           1308                               [▁I, ▁miss, ▁you, .]   \n",
       "\n",
       "   DstSentenceID                                        DstSentence  \n",
       "0         485968        [म्यूरियल, अब, बीस, साल, की, हो, गई, है, ।]  \n",
       "1        2060319                [म्यूरियल, अब, बीस, साल, की, है, ।]  \n",
       "2         485564  [मैं, इस, दुनिया, में, शिक्षा, पर, बहुत, निराश...  \n",
       "3        2060320                              [वैसा, नहीं, होगा, ।]  \n",
       "4        2060321              [मुझें, तुम्हारी, याद, आ, रही, है, ।]  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:29.917416Z",
     "iopub.status.busy": "2025-07-11T16:32:29.917229Z",
     "iopub.status.idle": "2025-07-11T16:32:29.997040Z",
     "shell.execute_reply": "2025-07-11T16:32:29.996438Z",
     "shell.execute_reply.started": "2025-07-11T16:32:29.917400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#converting English word to numerical\n",
    "data['SrcSentence'] = data['SrcSentence'].apply(tokenizer.convert_tokens_to_ids)\n",
    "\n",
    "#then after that english numeric value will ues as a input in NN afeter converting One hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:29.998198Z",
     "iopub.status.busy": "2025-07-11T16:32:29.997906Z",
     "iopub.status.idle": "2025-07-11T16:32:30.001949Z",
     "shell.execute_reply": "2025-07-11T16:32:30.001285Z",
     "shell.execute_reply.started": "2025-07-11T16:32:29.998171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#converting Hindi Sentence to numbers \n",
    "def convert_hindi_tokens_to_ids(tokenized_hindi_sentence):\n",
    "    return [Vd[token] for token in tokenized_hindi_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.002926Z",
     "iopub.status.busy": "2025-07-11T16:32:30.002737Z",
     "iopub.status.idle": "2025-07-11T16:32:30.035565Z",
     "shell.execute_reply": "2025-07-11T16:32:30.034799Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.002909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data['DstSentence'] = data['DstSentence'].apply(convert_hindi_tokens_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.036371Z",
     "iopub.status.busy": "2025-07-11T16:32:30.036188Z",
     "iopub.status.idle": "2025-07-11T16:32:30.049856Z",
     "shell.execute_reply": "2025-07-11T16:32:30.049049Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.036355Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SrcSentenceID</th>\n",
       "      <th>SrcSentence</th>\n",
       "      <th>DstSentenceID</th>\n",
       "      <th>DstSentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1282</td>\n",
       "      <td>[4159, 23, 14018, 19, 460, 230, 5]</td>\n",
       "      <td>485968</td>\n",
       "      <td>[4446, 1212, 811, 6742, 1997, 6567, 2573, 4117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1282</td>\n",
       "      <td>[4159, 23, 14018, 19, 460, 230, 5]</td>\n",
       "      <td>2060319</td>\n",
       "      <td>[4446, 1212, 811, 6742, 1997, 4117, 4344]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1294</td>\n",
       "      <td>[2855, 16, 48, 296, 26963, 7, 140, 5]</td>\n",
       "      <td>485564</td>\n",
       "      <td>[5968, 6994, 4152, 5941, 1115, 3143, 3963, 421...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1302</td>\n",
       "      <td>[466, 751, 31, 17, 1837, 5]</td>\n",
       "      <td>2060320</td>\n",
       "      <td>[2230, 4800, 6686, 4344]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1308</td>\n",
       "      <td>[27, 3041, 25, 5]</td>\n",
       "      <td>2060321</td>\n",
       "      <td>[4737, 3554, 6787, 3712, 501, 4117, 4344]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SrcSentenceID                            SrcSentence  DstSentenceID  \\\n",
       "0           1282     [4159, 23, 14018, 19, 460, 230, 5]         485968   \n",
       "1           1282     [4159, 23, 14018, 19, 460, 230, 5]        2060319   \n",
       "2           1294  [2855, 16, 48, 296, 26963, 7, 140, 5]         485564   \n",
       "3           1302            [466, 751, 31, 17, 1837, 5]        2060320   \n",
       "4           1308                      [27, 3041, 25, 5]        2060321   \n",
       "\n",
       "                                         DstSentence  \n",
       "0  [4446, 1212, 811, 6742, 1997, 6567, 2573, 4117...  \n",
       "1          [4446, 1212, 811, 6742, 1997, 4117, 4344]  \n",
       "2  [5968, 6994, 4152, 5941, 1115, 3143, 3963, 421...  \n",
       "3                           [2230, 4800, 6686, 4344]  \n",
       "4          [4737, 3554, 6787, 3712, 501, 4117, 4344]  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.050950Z",
     "iopub.status.busy": "2025-07-11T16:32:30.050666Z",
     "iopub.status.idle": "2025-07-11T16:32:30.065922Z",
     "shell.execute_reply": "2025-07-11T16:32:30.065221Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.050923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def insert_sos_token_id(hindi_sentence_token_ids_list):\n",
    "    return [1] + hindi_sentence_token_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.067044Z",
     "iopub.status.busy": "2025-07-11T16:32:30.066727Z",
     "iopub.status.idle": "2025-07-11T16:32:30.081554Z",
     "shell.execute_reply": "2025-07-11T16:32:30.080807Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.067016Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def insert_eos_token_id(hindi_sentence_token_ids_list):\n",
    "    return hindi_sentence_token_ids_list + [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.082566Z",
     "iopub.status.busy": "2025-07-11T16:32:30.082294Z",
     "iopub.status.idle": "2025-07-11T16:32:30.110666Z",
     "shell.execute_reply": "2025-07-11T16:32:30.109887Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.082538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data[\"DstSentenceInput\"] = data[\"DstSentence\"].apply(insert_sos_token_id)\n",
    "data[\"DstSentenceLabel\"] = data[\"DstSentence\"].apply(insert_eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.111709Z",
     "iopub.status.busy": "2025-07-11T16:32:30.111466Z",
     "iopub.status.idle": "2025-07-11T16:32:30.136028Z",
     "shell.execute_reply": "2025-07-11T16:32:30.135392Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.111683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SrcSentenceID</th>\n",
       "      <th>SrcSentence</th>\n",
       "      <th>DstSentenceID</th>\n",
       "      <th>DstSentence</th>\n",
       "      <th>DstSentenceInput</th>\n",
       "      <th>DstSentenceLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1282</td>\n",
       "      <td>[4159, 23, 14018, 19, 460, 230, 5]</td>\n",
       "      <td>485968</td>\n",
       "      <td>[4446, 1212, 811, 6742, 1997, 6567, 2573, 4117...</td>\n",
       "      <td>[1, 4446, 1212, 811, 6742, 1997, 6567, 2573, 4...</td>\n",
       "      <td>[4446, 1212, 811, 6742, 1997, 6567, 2573, 4117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1282</td>\n",
       "      <td>[4159, 23, 14018, 19, 460, 230, 5]</td>\n",
       "      <td>2060319</td>\n",
       "      <td>[4446, 1212, 811, 6742, 1997, 4117, 4344]</td>\n",
       "      <td>[1, 4446, 1212, 811, 6742, 1997, 4117, 4344]</td>\n",
       "      <td>[4446, 1212, 811, 6742, 1997, 4117, 4344, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1294</td>\n",
       "      <td>[2855, 16, 48, 296, 26963, 7, 140, 5]</td>\n",
       "      <td>485564</td>\n",
       "      <td>[5968, 6994, 4152, 5941, 1115, 3143, 3963, 421...</td>\n",
       "      <td>[1, 5968, 6994, 4152, 5941, 1115, 3143, 3963, ...</td>\n",
       "      <td>[5968, 6994, 4152, 5941, 1115, 3143, 3963, 421...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1302</td>\n",
       "      <td>[466, 751, 31, 17, 1837, 5]</td>\n",
       "      <td>2060320</td>\n",
       "      <td>[2230, 4800, 6686, 4344]</td>\n",
       "      <td>[1, 2230, 4800, 6686, 4344]</td>\n",
       "      <td>[2230, 4800, 6686, 4344, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1308</td>\n",
       "      <td>[27, 3041, 25, 5]</td>\n",
       "      <td>2060321</td>\n",
       "      <td>[4737, 3554, 6787, 3712, 501, 4117, 4344]</td>\n",
       "      <td>[1, 4737, 3554, 6787, 3712, 501, 4117, 4344]</td>\n",
       "      <td>[4737, 3554, 6787, 3712, 501, 4117, 4344, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SrcSentenceID                            SrcSentence  DstSentenceID  \\\n",
       "0           1282     [4159, 23, 14018, 19, 460, 230, 5]         485968   \n",
       "1           1282     [4159, 23, 14018, 19, 460, 230, 5]        2060319   \n",
       "2           1294  [2855, 16, 48, 296, 26963, 7, 140, 5]         485564   \n",
       "3           1302            [466, 751, 31, 17, 1837, 5]        2060320   \n",
       "4           1308                      [27, 3041, 25, 5]        2060321   \n",
       "\n",
       "                                         DstSentence  \\\n",
       "0  [4446, 1212, 811, 6742, 1997, 6567, 2573, 4117...   \n",
       "1          [4446, 1212, 811, 6742, 1997, 4117, 4344]   \n",
       "2  [5968, 6994, 4152, 5941, 1115, 3143, 3963, 421...   \n",
       "3                           [2230, 4800, 6686, 4344]   \n",
       "4          [4737, 3554, 6787, 3712, 501, 4117, 4344]   \n",
       "\n",
       "                                    DstSentenceInput  \\\n",
       "0  [1, 4446, 1212, 811, 6742, 1997, 6567, 2573, 4...   \n",
       "1       [1, 4446, 1212, 811, 6742, 1997, 4117, 4344]   \n",
       "2  [1, 5968, 6994, 4152, 5941, 1115, 3143, 3963, ...   \n",
       "3                        [1, 2230, 4800, 6686, 4344]   \n",
       "4       [1, 4737, 3554, 6787, 3712, 501, 4117, 4344]   \n",
       "\n",
       "                                    DstSentenceLabel  \n",
       "0  [4446, 1212, 811, 6742, 1997, 6567, 2573, 4117...  \n",
       "1       [4446, 1212, 811, 6742, 1997, 4117, 4344, 2]  \n",
       "2  [5968, 6994, 4152, 5941, 1115, 3143, 3963, 421...  \n",
       "3                        [2230, 4800, 6686, 4344, 2]  \n",
       "4       [4737, 3554, 6787, 3712, 501, 4117, 4344, 2]  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.136989Z",
     "iopub.status.busy": "2025-07-11T16:32:30.136733Z",
     "iopub.status.idle": "2025-07-11T16:32:30.153216Z",
     "shell.execute_reply": "2025-07-11T16:32:30.152442Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.136964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.drop(labels=[\"SrcSentenceID\",\"DstSentenceID\",\"DstSentence\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.154033Z",
     "iopub.status.busy": "2025-07-11T16:32:30.153854Z",
     "iopub.status.idle": "2025-07-11T16:32:30.181020Z",
     "shell.execute_reply": "2025-07-11T16:32:30.180381Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.154018Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SrcSentence</th>\n",
       "      <th>DstSentenceInput</th>\n",
       "      <th>DstSentenceLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4159, 23, 14018, 19, 460, 230, 5]</td>\n",
       "      <td>[1, 4446, 1212, 811, 6742, 1997, 6567, 2573, 4...</td>\n",
       "      <td>[4446, 1212, 811, 6742, 1997, 6567, 2573, 4117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4159, 23, 14018, 19, 460, 230, 5]</td>\n",
       "      <td>[1, 4446, 1212, 811, 6742, 1997, 4117, 4344]</td>\n",
       "      <td>[4446, 1212, 811, 6742, 1997, 4117, 4344, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2855, 16, 48, 296, 26963, 7, 140, 5]</td>\n",
       "      <td>[1, 5968, 6994, 4152, 5941, 1115, 3143, 3963, ...</td>\n",
       "      <td>[5968, 6994, 4152, 5941, 1115, 3143, 3963, 421...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[466, 751, 31, 17, 1837, 5]</td>\n",
       "      <td>[1, 2230, 4800, 6686, 4344]</td>\n",
       "      <td>[2230, 4800, 6686, 4344, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[27, 3041, 25, 5]</td>\n",
       "      <td>[1, 4737, 3554, 6787, 3712, 501, 4117, 4344]</td>\n",
       "      <td>[4737, 3554, 6787, 3712, 501, 4117, 4344, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             SrcSentence  \\\n",
       "0     [4159, 23, 14018, 19, 460, 230, 5]   \n",
       "1     [4159, 23, 14018, 19, 460, 230, 5]   \n",
       "2  [2855, 16, 48, 296, 26963, 7, 140, 5]   \n",
       "3            [466, 751, 31, 17, 1837, 5]   \n",
       "4                      [27, 3041, 25, 5]   \n",
       "\n",
       "                                    DstSentenceInput  \\\n",
       "0  [1, 4446, 1212, 811, 6742, 1997, 6567, 2573, 4...   \n",
       "1       [1, 4446, 1212, 811, 6742, 1997, 4117, 4344]   \n",
       "2  [1, 5968, 6994, 4152, 5941, 1115, 3143, 3963, ...   \n",
       "3                        [1, 2230, 4800, 6686, 4344]   \n",
       "4       [1, 4737, 3554, 6787, 3712, 501, 4117, 4344]   \n",
       "\n",
       "                                    DstSentenceLabel  \n",
       "0  [4446, 1212, 811, 6742, 1997, 6567, 2573, 4117...  \n",
       "1       [4446, 1212, 811, 6742, 1997, 4117, 4344, 2]  \n",
       "2  [5968, 6994, 4152, 5941, 1115, 3143, 3963, 421...  \n",
       "3                        [2230, 4800, 6686, 4344, 2]  \n",
       "4       [4737, 3554, 6787, 3712, 501, 4117, 4344, 2]  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.182043Z",
     "iopub.status.busy": "2025-07-11T16:32:30.181841Z",
     "iopub.status.idle": "2025-07-11T16:32:30.880824Z",
     "shell.execute_reply": "2025-07-11T16:32:30.880163Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.182025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = list(data[\"SrcSentence\"])\n",
    "Y_input = list(data[\"DstSentenceInput\"])\n",
    "Y_label = list(data[\"DstSentenceLabel\"])\n",
    "\n",
    "X_tensor = [torch.tensor(eng_tokenized_ids) for eng_tokenized_ids in X]\n",
    "Y_input_tensor = [torch.tensor(hin_tokenized_ids) for hin_tokenized_ids in Y_input]\n",
    "Y_label_tensor = [torch.tensor(hin_tokenized_ids) for hin_tokenized_ids in Y_label]\n",
    "\n",
    "X_padded = torch.nn.utils.rnn.pad_sequence(X_tensor,batch_first=True)\n",
    "Y_input_padded = torch.nn.utils.rnn.pad_sequence(Y_input_tensor,batch_first=True)\n",
    "Y_label_padded = torch.nn.utils.rnn.pad_sequence(Y_label_tensor,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.881793Z",
     "iopub.status.busy": "2025-07-11T16:32:30.881546Z",
     "iopub.status.idle": "2025-07-11T16:32:30.886938Z",
     "shell.execute_reply": "2025-07-11T16:32:30.886161Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.881772Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13182, 68])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.887972Z",
     "iopub.status.busy": "2025-07-11T16:32:30.887689Z",
     "iopub.status.idle": "2025-07-11T16:32:30.905960Z",
     "shell.execute_reply": "2025-07-11T16:32:30.905344Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.887946Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2855,    16,    48,   296, 26963,     7,   140,     5,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_padded[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.906920Z",
     "iopub.status.busy": "2025-07-11T16:32:30.906670Z",
     "iopub.status.idle": "2025-07-11T16:32:30.921366Z",
     "shell.execute_reply": "2025-07-11T16:32:30.920685Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.906890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seq_len = X_padded.shape[1]\n",
    "dest_len = Y_label_padded.shape[1]\n",
    "src_vocab_size = len(Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.922229Z",
     "iopub.status.busy": "2025-07-11T16:32:30.922005Z",
     "iopub.status.idle": "2025-07-11T16:32:30.938203Z",
     "shell.execute_reply": "2025-07-11T16:32:30.937431Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.922210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 68, 32100)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len, dest_len, src_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.939203Z",
     "iopub.status.busy": "2025-07-11T16:32:30.938914Z",
     "iopub.status.idle": "2025-07-11T16:32:30.953816Z",
     "shell.execute_reply": "2025-07-11T16:32:30.953071Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.939176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:30.954723Z",
     "iopub.status.busy": "2025-07-11T16:32:30.954482Z",
     "iopub.status.idle": "2025-07-11T16:32:31.375382Z",
     "shell.execute_reply": "2025-07-11T16:32:31.374415Z",
     "shell.execute_reply.started": "2025-07-11T16:32:30.954693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "d_model = 256\n",
    "src_embedding = torch.nn.Embedding(src_vocab_size, d_model)\n",
    "x = src_embedding(X_padded)  # (batch_size, seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:31.376596Z",
     "iopub.status.busy": "2025-07-11T16:32:31.376303Z",
     "iopub.status.idle": "2025-07-11T16:32:31.828388Z",
     "shell.execute_reply": "2025-07-11T16:32:31.827660Z",
     "shell.execute_reply.started": "2025-07-11T16:32:31.376569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "pos_encoder = PositionalEncoding(d_model, max_len=X_padded.shape[1])\n",
    "x = pos_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:31.829493Z",
     "iopub.status.busy": "2025-07-11T16:32:31.829202Z",
     "iopub.status.idle": "2025-07-11T16:32:31.836953Z",
     "shell.execute_reply": "2025-07-11T16:32:31.836074Z",
     "shell.execute_reply.started": "2025-07-11T16:32:31.829467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size \n",
    "        self.heads = heads # 8 in Transformer\n",
    "        self.head_dim = embed_size // heads \n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "        # === Project Embeddings into three vectors: Query, Key and Value ===\n",
    "        \n",
    "        self.values = torch.nn.Linear(embed_size, embed_size)\n",
    "        self.keys = torch.nn.Linear(embed_size, embed_size)\n",
    "        self.queries = torch.nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = torch.nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Values, Keys and Queries have size: (batch_size, sequence_len, embedding_size)\n",
    "        batch_size = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        # === Pass through Linear Layer ===\n",
    "        values = self.values(values)  # (batch_size, value_len, embed_size)\n",
    "        keys = self.keys(keys)  # (batch_size, key_len, embed_size)\n",
    "        queries = self.queries(query)  # (batch_size, query_len, embed_size)\n",
    "\n",
    "        values = values.reshape(batch_size, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(batch_size, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(batch_size, query_len, self.heads, self.head_dim)\n",
    "\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (batch_size, query_len, heads, heads_dim),\n",
    "        # keys shape: (batch_size, key_len, heads, heads_dim)\n",
    "        # energy: (batch_size, heads, query_len, key_len)\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for\n",
    "        # better stability\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3) \n",
    "        # attention shape: (batch_size, heads, query_len, key_len)\n",
    "        # values shape: (batch_size, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (batch_size, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            batch_size, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (batch_size, query_len, embed_size)\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:31.837964Z",
     "iopub.status.busy": "2025-07-11T16:32:31.837710Z",
     "iopub.status.idle": "2025-07-11T16:32:41.840235Z",
     "shell.execute_reply": "2025-07-11T16:32:41.839276Z",
     "shell.execute_reply.started": "2025-07-11T16:32:31.837934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "heads = 8\n",
    "self_attn = MultiHeadAttention(embed_size, 8)\n",
    "attn_output = self_attn(x,x,x,mask=None)  # Still (batch_size, seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:41.841471Z",
     "iopub.status.busy": "2025-07-11T16:32:41.841209Z",
     "iopub.status.idle": "2025-07-11T16:32:41.846641Z",
     "shell.execute_reply": "2025-07-11T16:32:41.845834Z",
     "shell.execute_reply.started": "2025-07-11T16:32:41.841442Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13182, 68, 256])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:32:41.847842Z",
     "iopub.status.busy": "2025-07-11T16:32:41.847530Z",
     "iopub.status.idle": "2025-07-11T16:32:41.864903Z",
     "shell.execute_reply": "2025-07-11T16:32:41.864139Z",
     "shell.execute_reply.started": "2025-07-11T16:32:41.847812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerLayer(torch.nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion=4):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads) \n",
    "        self.norm1 = torch.nn.LayerNorm(embed_size)\n",
    "        self.norm2 = torch.nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        # Values, Keys and Queries have size: (batch_size, query_len, embedding_size)\n",
    "        attention = self.attention(value, key, query, mask) # attention shape: (batch_size, query_len, embedding_size)\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query)) # x shape: (batch_size, query_len, embedding_size)\n",
    "        forward = self.feed_forward(x) # forward shape: (batch_size, query_len, embedding_size)\n",
    "        out = self.dropout(self.norm2(forward + x)) # out shape: (batch_size, query_len, embedding_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:40:44.344001Z",
     "iopub.status.busy": "2025-07-11T16:40:44.343678Z",
     "iopub.status.idle": "2025-07-11T16:40:44.350715Z",
     "shell.execute_reply": "2025-07-11T16:40:44.349839Z",
     "shell.execute_reply.started": "2025-07-11T16:40:44.343979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, src_vocab_size, embed_size, num_layers, heads,\n",
    "        device, forward_expansion, dropout, max_length): \n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size # size of the input embedding\n",
    "        self.device = device # either \"cuda\" or \"cpu\"\n",
    "        # Lookup table with an embedding for each word in the vocabulary\n",
    "        self.word_embedding = torch.nn.Embedding(src_vocab_size, embed_size) \n",
    "        # Lookup table with a positional embedding for each word in the sequence\n",
    "        self.position_embedding =torch.nn.Embedding(max_length, embed_size)\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                TransformerLayer(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "       \n",
    "        batch_size, seq_length = x.shape\n",
    "        x = x.to(self.device)\n",
    "        # positions is an arange from (0,seq_len), e.g: torch.tensor([[0,1,2,...,N], [0,1,2,...,N], ..., [0,1,2,...,N]])\n",
    "        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(self.device)\n",
    "        x = x.long()\n",
    "        \n",
    "        out = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "        # In the Encoder the query, key, value are all the same, in the\n",
    "        # decoder this will change. This might look a bit odd in this case.\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "        # output shape: torch.Size([batch_size, sequence_length, embedding_size])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:40:45.806286Z",
     "iopub.status.busy": "2025-07-11T16:40:45.805958Z",
     "iopub.status.idle": "2025-07-11T16:40:45.811422Z",
     "shell.execute_reply": "2025-07-11T16:40:45.810557Z",
     "shell.execute_reply.started": "2025-07-11T16:40:45.806259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.norm = torch.nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerLayer(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:40:46.520220Z",
     "iopub.status.busy": "2025-07-11T16:40:46.519855Z",
     "iopub.status.idle": "2025-07-11T16:40:46.526668Z",
     "shell.execute_reply": "2025-07-11T16:40:46.525709Z",
     "shell.execute_reply.started": "2025-07-11T16:40:46.520189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, forward_expansion,\n",
    "        dropout, device, max_length):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        #=== For each token in target vocab there is a token embedding ===\n",
    "        \n",
    "        self.word_embedding = torch.nn.Embedding(trg_vocab_size, embed_size) \n",
    "        self.position_embedding = torch.nn.Embedding(max_length, embed_size)\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = torch.nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        :param x: target sequence. Shape: (batch_size, target_sequence_len)\n",
    "        :param enc_out: encoder output. Shape: (batch_size, src_sequence_length, embedding_size)\n",
    "        :param src_mask: source mask.\n",
    "        :param trg_mask: target mask.\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = x.shape # x shape: (batch_size, target_sequence_len)\n",
    "        # positions is an arange from (0,seq_len), e.g: torch.tensor([[0,1,2,...,N], [0,1,2,...,N], ..., [0,1,2,...,N]])\n",
    "        x = x.to(self.device)\n",
    "        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(self.device) # positions shape: (batch_size, target_sequence_len)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "    \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:40:47.581663Z",
     "iopub.status.busy": "2025-07-11T16:40:47.581370Z",
     "iopub.status.idle": "2025-07-11T16:40:47.588922Z",
     "shell.execute_reply": "2025-07-11T16:40:47.588192Z",
     "shell.execute_reply.started": "2025-07-11T16:40:47.581640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=512,num_layers=6, forward_expansion=4, heads=8, dropout=0, device=device, max_length=100):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "        # === Encoder ===\n",
    "        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length)\n",
    "        # === Decoder ===\n",
    "        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "        \n",
    "    def make_trg_mask(self, trg):\n",
    "        # trg: (N, trg_len)\n",
    "        N, trg_len = trg.shape\n",
    "\n",
    "        # Padding mask: (N, 1, 1, trg_len)\n",
    "        pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2).to(self.device)\n",
    "\n",
    "        # Causal mask: (1, 1, trg_len, trg_len)\n",
    "        causal_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        # Combine: only attend to previous tokens and non-pad\n",
    "        trg_mask = pad_mask & causal_mask  # shape: (N, 1, trg_len, trg_len)\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src) # src_mask shape: \n",
    "        trg_mask = self.make_trg_mask(trg) # trg_mask shape: \n",
    "        enc_src = self.encoder(src, src_mask) # enc_src shape:\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask) # out shape: \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:40:49.064836Z",
     "iopub.status.busy": "2025-07-11T16:40:49.064518Z",
     "iopub.status.idle": "2025-07-11T16:40:49.068446Z",
     "shell.execute_reply": "2025-07-11T16:40:49.067667Z",
     "shell.execute_reply.started": "2025-07-11T16:40:49.064808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "src = X_padded           # (batch_size, src_seq_len)\n",
    "trg = Y_input_padded     # (batch_size, trg_seq_len)\n",
    "labels = Y_label_padded  # (batch_size, trg_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:40:50.007556Z",
     "iopub.status.busy": "2025-07-11T16:40:50.007267Z",
     "iopub.status.idle": "2025-07-11T16:40:50.012873Z",
     "shell.execute_reply": "2025-07-11T16:40:50.012006Z",
     "shell.execute_reply.started": "2025-07-11T16:40:50.007532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13182, 68]), torch.Size([13182, 68]), torch.Size([13182, 68]))"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_padded.shape, Y_input_padded.shape, Y_label_padded.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:40:50.665906Z",
     "iopub.status.busy": "2025-07-11T16:40:50.665624Z",
     "iopub.status.idle": "2025-07-11T16:40:50.670453Z",
     "shell.execute_reply": "2025-07-11T16:40:50.669614Z",
     "shell.execute_reply.started": "2025-07-11T16:40:50.665886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_padded_train = X_padded[0:13000]\n",
    "Y_input_padded_train = Y_input_padded[0:13000]\n",
    "Y_label_padded_train = Y_label_padded[0:13000]\n",
    "\n",
    "X_padded_test = X_padded[13000:]\n",
    "Y_input_padded_test = Y_input_padded[13000:]\n",
    "Y_label_padded_test = Y_label_padded[13000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:40:51.384667Z",
     "iopub.status.busy": "2025-07-11T16:40:51.384389Z",
     "iopub.status.idle": "2025-07-11T16:40:51.394059Z",
     "shell.execute_reply": "2025-07-11T16:40:51.393362Z",
     "shell.execute_reply.started": "2025-07-11T16:40:51.384646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Move training data to device\n",
    "X_padded_train = X_padded_train.to(device)\n",
    "Y_input_padded_train = Y_input_padded_train.to(device)\n",
    "Y_label_padded_train = Y_label_padded_train.to(device)\n",
    "\n",
    "# Move test data to device\n",
    "X_padded_test = X_padded_test.to(device)\n",
    "Y_input_padded_test = Y_input_padded_test.to(device)\n",
    "Y_label_padded_test = Y_label_padded_test.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:40:51.776336Z",
     "iopub.status.busy": "2025-07-11T16:40:51.775996Z",
     "iopub.status.idle": "2025-07-11T16:40:52.002121Z",
     "shell.execute_reply": "2025-07-11T16:40:52.001168Z",
     "shell.execute_reply.started": "2025-07-11T16:40:51.776309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Hyperparameters\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64\n",
    "num_epochs = 35\n",
    "clip = 1\n",
    "trg_pad_idx = 0\n",
    "src_pad_idx = 0\n",
    "trg_vocab_size = len(Vd)\n",
    "# Loss and optimizer\n",
    "pad_idx = trg_pad_idx\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "src_vocab_size = len(eng_vocab)\n",
    "# Instantiate the model\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    trg_vocab_size=trg_vocab_size,\n",
    "    src_pad_idx=src_pad_idx,\n",
    "    trg_pad_idx=trg_pad_idx,\n",
    "    embed_size=256,\n",
    "    num_layers=6,\n",
    "    forward_expansion=6,\n",
    "    heads=8,\n",
    "    dropout=0.3,\n",
    "    device=device,\n",
    "    max_length=100\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:40:53.207722Z",
     "iopub.status.busy": "2025-07-11T16:40:53.207434Z",
     "iopub.status.idle": "2025-07-11T16:40:53.217898Z",
     "shell.execute_reply": "2025-07-11T16:40:53.217042Z",
     "shell.execute_reply.started": "2025-07-11T16:40:53.207700Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "X_batch shape: torch.Size([64, 68])\n",
      "Y_input_batch shape: torch.Size([64, 68])\n",
      "Y_label_batch shape: torch.Size([64, 68])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Create TensorDataset for training\n",
    "train_dataset = TensorDataset(X_padded_train, Y_input_padded_train, Y_label_padded_train)\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "# Create TensorDataset for testing\n",
    "test_dataset = TensorDataset(X_padded_test, Y_input_padded_test, Y_label_padded_test)\n",
    "\n",
    "# Create DataLoader for testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "# Optionally, printing some examples from the train loader to verify\n",
    "for batch_idx, (X_batch, Y_input_batch, Y_label_batch) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(\"X_batch shape:\", X_batch.shape)\n",
    "    print(\"Y_input_batch shape:\", Y_input_batch.shape)\n",
    "    print(\"Y_label_batch shape:\", Y_label_batch.shape)\n",
    "    break  # Just look at the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:40:53.502902Z",
     "iopub.status.busy": "2025-07-11T16:40:53.502596Z",
     "iopub.status.idle": "2025-07-11T16:40:53.507572Z",
     "shell.execute_reply": "2025-07-11T16:40:53.506592Z",
     "shell.execute_reply.started": "2025-07-11T16:40:53.502875Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "divice  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:40:54.211648Z",
     "iopub.status.busy": "2025-07-11T16:40:54.211339Z",
     "iopub.status.idle": "2025-07-11T16:40:54.445733Z",
     "shell.execute_reply": "2025-07-11T16:40:54.444221Z",
     "shell.execute_reply.started": "2025-07-11T16:40:54.211620Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[13182, 68, 8, 32]' is invalid for input of size 1114112",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-270-add748ef2bcd>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Output shape: (batch_size, trg_len, vocab_size) → (batch_size * trg_len, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Target shape: (batch_size, trg_len) → (batch_size * trg_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shift trg for teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-262-28b8888a2be3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_trg_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# trg_mask shape:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0menc_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# enc_src shape:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# out shape:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-261-7d63df859e99>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, enc_out, src_mask, trg_mask)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-260-6a4418873b3e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, value, key, src_mask, trg_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-237-c155694d285f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, value, key, query, mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Values, Keys and Queries have size: (batch_size, query_len, embedding_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# attention shape: (batch_size, query_len, embedding_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Add skip connection, run through normalization and finally dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# x shape: (batch_size, query_len, embedding_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-234-066a6c90a1dd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, values, keys, query, mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, query_len, embed_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[13182, 68, 8, 32]' is invalid for input of size 1114112"
     ]
    }
   ],
   "source": [
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for idx, (src, trg_input, trg_label) in enumerate(train_loader):\n",
    "        src = src.to(device)\n",
    "        trg_input = trg_input.to(device)\n",
    "        trg_label = trg_label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg_input)\n",
    "\n",
    "        # Reshape for loss calculation: \n",
    "        # Output shape: (batch_size, trg_len, vocab_size) → (batch_size * trg_len, vocab_size)\n",
    "        # Target shape: (batch_size, trg_len) → (batch_size * trg_len)\n",
    "        output = model(src, trg[:, :-1])  # shift trg for teacher forcing\n",
    "\n",
    "        \n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        \n",
    "        trg_label = trg_label.reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg_label)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if idx % 500 == 0:\n",
    "            print(f\"Epochs {epoch+1}/ {num_epochs} and loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-11T16:32:42.395804Z",
     "iopub.status.idle": "2025-07-11T16:32:42.396073Z",
     "shell.execute_reply": "2025-07-11T16:32:42.395966Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenize(sentence):\n",
    "    return re.findall(r\"\\b\\w+\\b\", sentence.lower())\n",
    "\n",
    "#inference loop\n",
    "def translate(sentence,model,eng_vocab, hin_vocab, inv_hindi_vocab, max_len=50):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    tokens = tokenize(sentence)\n",
    "    print(tokens)\n",
    "    #tokens = [tok for tok in tokens if tok in eng_vocab]\n",
    "    #input_tensor = torch.tensor([tok eng_vocab[\"<UNK>\"] for tok in tokens]).to(device)\n",
    "    print(\"<UNK>\" in eng_vocab)   # should print True\n",
    "    print(eng_vocab.get(\"<UNK>\", \"Not Found\"))  # should print 3 or the correct index\n",
    "    unk_count = sum(1 for tok in tokens if tok not in eng_vocab)\n",
    "    print(f\"Total UNKs in input: {unk_count} out of {len(tokens)}\")\n",
    "    input_tensor = torch.tensor([eng_vocab.get(tok, eng_vocab[\"<UNK>\"]) for tok in tokens]).unsqueeze(0).to(device)\n",
    " \n",
    "    #generate souce masking\n",
    "    src_mask = model.make_src_mask(input_tensor)\n",
    "\n",
    "#passing through encoder\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(input_tensor,src_mask)\n",
    "\n",
    "#start with SOS token\n",
    "    trg_indices = [hin_vocab[\"<SOS>\"]]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.tensor(trg_indices).unsqueeze(0).to(device)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.decoder(trg_tensor, enc_src, src_mask, trg_mask)\n",
    "\n",
    "        next_token = output.argmax(-1)[:,-1].item() #picking most probable next token\n",
    "\n",
    "        trg_indices.append(next_token)\n",
    "\n",
    "        if next_token == hin_vocab[\"<EOS>\"]:\n",
    "            break\n",
    "    translated_sent = [inv_hindi_vocab.get(idx, \"<UNK>\") for idx in trg_indices[1:]]\n",
    "        \n",
    "    return \" \".join(translated_sent).replace(\"<EOS>\",\"\") #not including <SOS>\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-11T16:32:42.397178Z",
     "iopub.status.idle": "2025-07-11T16:32:42.397455Z",
     "shell.execute_reply": "2025-07-11T16:32:42.397342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inv_hindi_vocab = {index: word for word, index in Vd.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-11T16:32:42.398215Z",
     "iopub.status.idle": "2025-07-11T16:32:42.398559Z",
     "shell.execute_reply": "2025-07-11T16:32:42.398425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eng_input = \"is\"\n",
    "hindi_output = translate(eng_input, model, eng_vocab, Vd, inv_hindi_vocab)\n",
    "print(\"Translated:\", hindi_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6998524,
     "sourceId": 11208178,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7343205,
     "sourceId": 11699108,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
