{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nfrom collections import Counter\nimport re\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:08:06.689961Z","iopub.execute_input":"2025-09-25T07:08:06.690231Z","iopub.status.idle":"2025-09-25T07:08:08.218527Z","shell.execute_reply.started":"2025-09-25T07:08:06.690209Z","shell.execute_reply":"2025-09-25T07:08:08.217827Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"data = load_dataset(\"cfilt/iitb-english-hindi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:08:08.219359Z","iopub.execute_input":"2025-09-25T07:08:08.219730Z","iopub.status.idle":"2025-09-25T07:08:26.665444Z","shell.execute_reply.started":"2025-09-25T07:08:08.219712Z","shell.execute_reply":"2025-09-25T07:08:26.664925Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4150b04abc74aeaa7ec96883590aad7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c09e6e73677246d8976f5cd2320bb954"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/190M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2654ca2e9924fcda48e4907fbaa71de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/85.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fac471a26a9e4c16a8e05598eb08fcd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cd552bf7d644351a28b86a496182cf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1659083 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29a70efbae3b46a5b3c2e73567f3d13e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/520 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fa3f73fd688487a92217f5f8885b115"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2507 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f308390aad84181ae2681562f596f63"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:08:26.667120Z","iopub.execute_input":"2025-09-25T07:08:26.667437Z","iopub.status.idle":"2025-09-25T07:08:26.672503Z","shell.execute_reply.started":"2025-09-25T07:08:26.667419Z","shell.execute_reply":"2025-09-25T07:08:26.671681Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['translation'],\n        num_rows: 1659083\n    })\n    validation: Dataset({\n        features: ['translation'],\n        num_rows: 520\n    })\n    test: Dataset({\n        features: ['translation'],\n        num_rows: 2507\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Convert into lists\nenglish_sentences = [x['en'] for x in data['train']['translation']]\nhindi_sentences   = [x['hi'] for x in data['train']['translation']]\nenglish_sentences_val = [x['en'] for x in data['validation']['translation']]\nhindi_sentences_val   = [x['hi'] for x in data['validation']['translation']]\n\nprint(\"English:\", english_sentences[:5])\nprint(\"Hindi:\", hindi_sentences[:5])\nprint(\"Hindi:\", len(hindi_sentences))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:08:26.673252Z","iopub.execute_input":"2025-09-25T07:08:26.673476Z","iopub.status.idle":"2025-09-25T07:08:46.480330Z","shell.execute_reply.started":"2025-09-25T07:08:26.673454Z","shell.execute_reply":"2025-09-25T07:08:46.479683Z"}},"outputs":[{"name":"stdout","text":"English: ['Give your application an accessibility workout', 'Accerciser Accessibility Explorer', 'The default plugin layout for the bottom panel', 'The default plugin layout for the top panel', 'A list of plugins that are disabled by default']\nHindi: ['अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें', 'एक्सेर्साइसर पहुंचनीयता अन्वेषक', 'निचले पटल के लिए डिफोल्ट प्लग-इन खाका', 'ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका', 'उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से निष्क्रिय किया गया है']\nHindi: 1659083\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(\"Hindi:\", len(hindi_sentences_val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:08:46.480988Z","iopub.execute_input":"2025-09-25T07:08:46.481173Z","iopub.status.idle":"2025-09-25T07:08:46.485240Z","shell.execute_reply.started":"2025-09-25T07:08:46.481159Z","shell.execute_reply":"2025-09-25T07:08:46.484544Z"}},"outputs":[{"name":"stdout","text":"Hindi: 520\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n\ndef train_tokenizer(sentences, vocab_size=30000, lang=\"en\"):\n    tokenizer = Tokenizer(models.BPE())\n    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n    trainer = trainers.BpeTrainer(\n        vocab_size=vocab_size,\n        special_tokens=[\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"]\n    )\n    tokenizer.train_from_iterator(sentences, trainer)\n    tokenizer.save(f\"{lang}_tokenizer.json\")\n    return tokenizer\n\n# Collect sentences\nenglish_sentences = [x['translation']['en'] for x in data['train']]\nhindi_sentences   = [x['translation']['hi'] for x in data['train']]\n\nsrc_tokenizer = train_tokenizer(english_sentences, lang=\"en\")\ntrg_tokenizer = train_tokenizer(hindi_sentences, lang=\"hi\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:08:46.485936Z","iopub.execute_input":"2025-09-25T07:08:46.486154Z","iopub.status.idle":"2025-09-25T07:10:30.170456Z","shell.execute_reply.started":"2025-09-25T07:08:46.486129Z","shell.execute_reply":"2025-09-25T07:10:30.169669Z"}},"outputs":[{"name":"stdout","text":"\n\n\n\n\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"src_tokenizer = Tokenizer.from_file(\"en_tokenizer.json\")\ntrg_tokenizer = Tokenizer.from_file(\"hi_tokenizer.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:10:30.172073Z","iopub.execute_input":"2025-09-25T07:10:30.172361Z","iopub.status.idle":"2025-09-25T07:10:30.448188Z","shell.execute_reply.started":"2025-09-25T07:10:30.172337Z","shell.execute_reply":"2025-09-25T07:10:30.447457Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass TranslationDataset(Dataset):\n    def __init__(self, data, src_tokenizer, trg_tokenizer, max_len=50):\n        self.data = data\n        self.src_tok = src_tokenizer\n        self.trg_tok = trg_tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        idx = int(idx)\n        src = self.data[idx]['translation']['en']\n        trg = self.data[idx]['translation']['hi']\n\n        # Encode\n        src_ids = [self.src_tok.token_to_id(\"<SOS>\")] + self.src_tok.encode(src).ids[:self.max_len-2] + [self.src_tok.token_to_id(\"<EOS>\")]\n        trg_ids = [self.trg_tok.token_to_id(\"<SOS>\")] + self.trg_tok.encode(trg).ids[:self.max_len-2] + [self.trg_tok.token_to_id(\"<EOS>\")]\n\n        return torch.tensor(src_ids), torch.tensor(trg_ids)\n\ntrain_data = list(data['train'])\ntest_data  = list(data['test'])\n\ntrain_dataset = TranslationDataset(data['train'], src_tokenizer, trg_tokenizer)\ntest_dataset  = TranslationDataset(data['test'], src_tokenizer, trg_tokenizer)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:10:30.456555Z","iopub.execute_input":"2025-09-25T07:10:30.456822Z","iopub.status.idle":"2025-09-25T07:11:11.406850Z","shell.execute_reply.started":"2025-09-25T07:10:30.456806Z","shell.execute_reply":"2025-09-25T07:11:11.406046Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"val_dataset  = TranslationDataset(data['validation'], src_tokenizer, trg_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:11:11.409516Z","iopub.execute_input":"2025-09-25T07:11:11.409747Z","iopub.status.idle":"2025-09-25T07:11:11.413385Z","shell.execute_reply.started":"2025-09-25T07:11:11.409731Z","shell.execute_reply":"2025-09-25T07:11:11.412669Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\npad_idx = src_tokenizer.token_to_id(\"<PAD>\")\n\ndef collate_fn(batch):\n    src_batch, trg_batch = zip(*batch)\n    src_batch = pad_sequence(src_batch, padding_value=src_tokenizer.token_to_id(\"<PAD>\"), batch_first=True)\n    trg_batch = pad_sequence(trg_batch, padding_value=trg_tokenizer.token_to_id(\"<PAD>\"), batch_first=True)\n    return src_batch, trg_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:11:11.471161Z","iopub.execute_input":"2025-09-25T07:11:11.471442Z","iopub.status.idle":"2025-09-25T07:11:11.483352Z","shell.execute_reply.started":"2025-09-25T07:11:11.471426Z","shell.execute_reply":"2025-09-25T07:11:11.482776Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers=1, dropout=0.3):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=0)\n        self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))   # [batch, src_len, embed_dim]\n        outputs, hidden = self.rnn(embedded)           # outputs: [batch, src_len, hidden_dim]\n        return outputs, hidden\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:11:11.484114Z","iopub.execute_input":"2025-09-25T07:11:11.484335Z","iopub.status.idle":"2025-09-25T07:11:11.500564Z","shell.execute_reply.started":"2025-09-25T07:11:11.484319Z","shell.execute_reply":"2025-09-25T07:11:11.499953Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.v = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs):\n        # hidden: [1, batch, hidden_dim]\n        hidden = hidden[-1].unsqueeze(1)               # [batch, 1, hidden_dim]\n        src_len = encoder_outputs.size(1)\n\n        hidden = hidden.repeat(1, src_len, 1)          # [batch, src_len, hidden_dim]\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n        attention = self.v(energy).squeeze(2)          # [batch, src_len]\n\n        return torch.softmax(attention, dim=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:11:11.501264Z","iopub.execute_input":"2025-09-25T07:11:11.501444Z","iopub.status.idle":"2025-09-25T07:11:11.513329Z","shell.execute_reply.started":"2025-09-25T07:11:11.501431Z","shell.execute_reply":"2025-09-25T07:11:11.512676Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, output_dim, embed_dim, hidden_dim, attention, num_layers=1, dropout=0.3):\n        super().__init__()\n        self.output_dim = output_dim\n        self.attention = attention\n\n        self.embedding = nn.Embedding(output_dim, embed_dim, padding_idx=0)\n        self.rnn = nn.GRU(hidden_dim + embed_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n        self.fc = nn.Linear(hidden_dim * 2 + embed_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, encoder_outputs):\n        input = input.unsqueeze(1)                     # [batch, 1]\n        embedded = self.dropout(self.embedding(input)) # [batch, 1, embed_dim]\n\n        # Attention\n        attn_weights = self.attention(hidden, encoder_outputs)   # [batch, src_len]\n        attn_weights = attn_weights.unsqueeze(1)                 # [batch, 1, src_len]\n\n        context = torch.bmm(attn_weights, encoder_outputs)       # [batch, 1, hidden_dim]\n\n        rnn_input = torch.cat((embedded, context), dim=2)        # [batch, 1, embed+hidden]\n        output, hidden = self.rnn(rnn_input, hidden)\n\n        prediction = self.fc(torch.cat((output, context, embedded), dim=2).squeeze(1))\n        return prediction, hidden\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:11:11.513947Z","iopub.execute_input":"2025-09-25T07:11:11.514157Z","iopub.status.idle":"2025-09-25T07:11:11.528137Z","shell.execute_reply.started":"2025-09-25T07:11:11.514143Z","shell.execute_reply":"2025-09-25T07:11:11.527481Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#encoder + attention + decoder\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = src.size(0)\n        trg_len = trg.size(1)\n        trg_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n\n        encoder_outputs, hidden = self.encoder(src)\n        input = trg[:, 0]   # <SOS> token\n\n        for t in range(1, trg_len):\n            output, hidden = self.decoder(input, hidden, encoder_outputs)\n            outputs[:, t, :] = output\n\n            top1 = output.argmax(1)\n            input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n\n        return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:11:11.528947Z","iopub.execute_input":"2025-09-25T07:11:11.529209Z","iopub.status.idle":"2025-09-25T07:11:11.545811Z","shell.execute_reply.started":"2025-09-25T07:11:11.529190Z","shell.execute_reply":"2025-09-25T07:11:11.545157Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"INPUT_DIM = src_tokenizer.get_vocab_size()    # English vocab size\nOUTPUT_DIM = trg_tokenizer.get_vocab_size()    # Hindi vocab size\nEMBED_DIM = 256\nHIDDEN_DIM = 512\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nattn = Attention(HIDDEN_DIM)\nenc = Encoder(INPUT_DIM, EMBED_DIM, HIDDEN_DIM)\ndec = Decoder(OUTPUT_DIM, EMBED_DIM, HIDDEN_DIM, attn)\n\n\n\nmodel = Seq2Seq(enc, dec, device).to(device)\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:11:11.546461Z","iopub.execute_input":"2025-09-25T07:11:11.546719Z","iopub.status.idle":"2025-09-25T07:11:12.451372Z","shell.execute_reply.started":"2025-09-25T07:11:11.546701Z","shell.execute_reply":"2025-09-25T07:11:12.450768Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(30000, 256, padding_idx=0)\n    (rnn): GRU(256, 512, batch_first=True, dropout=0.3)\n    (dropout): Dropout(p=0.3, inplace=False)\n  )\n  (decoder): Decoder(\n    (attention): Attention(\n      (attn): Linear(in_features=1024, out_features=512, bias=True)\n      (v): Linear(in_features=512, out_features=1, bias=False)\n    )\n    (embedding): Embedding(30000, 256, padding_idx=0)\n    (rnn): GRU(768, 512, batch_first=True, dropout=0.3)\n    (fc): Linear(in_features=1280, out_features=30000, bias=True)\n    (dropout): Dropout(p=0.3, inplace=False)\n  )\n)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Subset\nimport numpy as np\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:11:12.452064Z","iopub.execute_input":"2025-09-25T07:11:12.452274Z","iopub.status.idle":"2025-09-25T07:11:12.455818Z","shell.execute_reply.started":"2025-09-25T07:11:12.452250Z","shell.execute_reply":"2025-09-25T07:11:12.455217Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def get_subset(dataset, n_samples,seed=42):\n    np.random.seed(seed)\n    subset_indices = np.random.choice(len(dataset), n_samples, replace=False)\n    return Subset(dataset, subset_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:11:12.456542Z","iopub.execute_input":"2025-09-25T07:11:12.456825Z","iopub.status.idle":"2025-09-25T07:11:12.470721Z","shell.execute_reply.started":"2025-09-25T07:11:12.456798Z","shell.execute_reply":"2025-09-25T07:11:12.469970Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"train_dataset_small = get_subset(train_dataset, n_samples=100000)  # 100k pairs\n#val_dataset_small = get_subset(val_dataset, n_samples=5000)        # 5k pairs\n\ntrain_loader = DataLoader(train_dataset_small, batch_size=64, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False,collate_fn=collate_fn)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_fn\n)\nprint(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:11:12.471718Z","iopub.execute_input":"2025-09-25T07:11:12.471949Z","iopub.status.idle":"2025-09-25T07:11:12.529197Z","shell.execute_reply.started":"2025-09-25T07:11:12.471931Z","shell.execute_reply":"2025-09-25T07:11:12.528669Z"}},"outputs":[{"name":"stdout","text":"Train batches: 1563, Val batches: 9\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"#pad_idx = hi_word2idx[\"<PAD>\"]\npad_idx = src_tokenizer.token_to_id(\"<PAD>\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=pad_idx,label_smoothing=0.1)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nscaler = torch.cuda.amp.GradScaler()\naccum_steps = 4  # simulate batch size 128\nsave_path = \"checkpoints\"\nos.makedirs(save_path, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:11:12.529801Z","iopub.execute_input":"2025-09-25T07:11:12.529994Z","iopub.status.idle":"2025-09-25T07:11:14.839037Z","shell.execute_reply.started":"2025-09-25T07:11:12.529978Z","shell.execute_reply":"2025-09-25T07:11:14.838363Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1818958519.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def train_model(num_epochs):\n    best_val_loss = float(\"inf\")\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0.0\n\n        for i, (src, trg) in enumerate(train_loader):\n            src, trg = src.to(device), trg.to(device)\n\n            optimizer.zero_grad()\n\n            with torch.cuda.amp.autocast():\n                output = model(src, trg[:, :-1])  # shift target\n                loss = criterion(output.reshape(-1, output.shape[-1]), trg[:, 1:].reshape(-1))\n\n            loss = loss / accum_steps\n            scaler.scale(loss).backward()\n\n            if (i + 1) % accum_steps == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n\n            total_loss += loss.item()\n\n            # Print progress every 500 steps\n            if (i + 1) % 500 == 0:\n                print(f\"Epoch {epoch+1}, Step {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n\n        avg_loss = total_loss / len(train_loader)\n        val_loss = evaluate(val_loader)\n        print(f\"Epoch {epoch+1} Complete | Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}\")\n\n        # Save checkpoint if improved\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), os.path.join(save_path, f\"best_model_epoch{epoch+1}.pt\"))\n            print(\"Saved new best model!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:11:14.839732Z","iopub.execute_input":"2025-09-25T07:11:14.840152Z","iopub.status.idle":"2025-09-25T07:11:14.846688Z","shell.execute_reply.started":"2025-09-25T07:11:14.840129Z","shell.execute_reply":"2025-09-25T07:11:14.845926Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def evaluate(loader):\n    model.eval()\n    total_loss = 0.0\n    with torch.no_grad():\n        for src, trg in loader:\n            src, trg = src.to(device), trg.to(device)\n            with torch.cuda.amp.autocast():\n                output = model(src, trg[:, :-1])\n                loss = criterion(output.reshape(-1, output.shape[-1]), trg[:, 1:].reshape(-1))\n            total_loss += loss.item()\n    return total_loss / len(loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:11:14.847447Z","iopub.execute_input":"2025-09-25T07:11:14.847995Z","iopub.status.idle":"2025-09-25T07:11:14.867363Z","shell.execute_reply.started":"2025-09-25T07:11:14.847968Z","shell.execute_reply":"2025-09-25T07:11:14.866801Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"train_model(num_epochs=5) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T07:11:14.868072Z","iopub.execute_input":"2025-09-25T07:11:14.868292Z","iopub.status.idle":"2025-09-25T08:35:50.494468Z","shell.execute_reply.started":"2025-09-25T07:11:14.868276Z","shell.execute_reply":"2025-09-25T08:35:50.493822Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3014641920.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Step 500/1563, Loss: 1.9456\nEpoch 1, Step 1000/1563, Loss: 1.8956\nEpoch 1, Step 1500/1563, Loss: 1.8961\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3438577531.py:7: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Complete | Train Loss: 1.9100, Val Loss: 7.7220\n✅ Saved new best model!\nEpoch 2, Step 500/1563, Loss: 1.8354\nEpoch 2, Step 1000/1563, Loss: 1.8115\nEpoch 2, Step 1500/1563, Loss: 1.7707\nEpoch 2 Complete | Train Loss: 1.8018, Val Loss: 7.5941\n✅ Saved new best model!\nEpoch 3, Step 500/1563, Loss: 1.7408\nEpoch 3, Step 1000/1563, Loss: 1.6914\nEpoch 3, Step 1500/1563, Loss: 1.7792\nEpoch 3 Complete | Train Loss: 1.7434, Val Loss: 7.4863\n✅ Saved new best model!\nEpoch 4, Step 500/1563, Loss: 1.6994\nEpoch 4, Step 1000/1563, Loss: 1.6779\nEpoch 4, Step 1500/1563, Loss: 1.7028\nEpoch 4 Complete | Train Loss: 1.6968, Val Loss: 7.4365\n✅ Saved new best model!\nEpoch 5, Step 500/1563, Loss: 1.7292\nEpoch 5, Step 1000/1563, Loss: 1.6581\nEpoch 5, Step 1500/1563, Loss: 1.5655\nEpoch 5 Complete | Train Loss: 1.6571, Val Loss: 7.4160\n✅ Saved new best model!\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\n\ndef translate_sentence(sentence):\n    model.eval()\n    src_ids = [src_tokenizer.token_to_id(\"<SOS>\")] + src_tokenizer.encode(sentence).ids + [src_tokenizer.token_to_id(\"<EOS>\")]\n    src_tensor = torch.tensor(src_ids).unsqueeze(0).to(device)\n\n    encoder_outputs, hidden = model.encoder(src_tensor)\n    input = torch.tensor([trg_tokenizer.token_to_id(\"<SOS>\")]).to(device)\n\n    result = []\n    for _ in range(50):\n        output, hidden = model.decoder(input, hidden, encoder_outputs)\n        pred_token = output.argmax(1).item()\n        if pred_token == trg_tokenizer.token_to_id(\"<EOS>\"):\n            break\n        result.append(pred_token)\n        input = torch.tensor([pred_token]).to(device)\n\n    return trg_tokenizer.decode(result)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:36:01.514605Z","iopub.execute_input":"2025-09-25T08:36:01.514900Z","iopub.status.idle":"2025-09-25T08:36:01.520782Z","shell.execute_reply.started":"2025-09-25T08:36:01.514881Z","shell.execute_reply":"2025-09-25T08:36:01.520046Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"print(translate_sentence(\"how are you\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:36:32.504436Z","iopub.execute_input":"2025-09-25T08:36:32.505010Z","iopub.status.idle":"2025-09-25T08:36:32.516567Z","shell.execute_reply.started":"2025-09-25T08:36:32.504985Z","shell.execute_reply":"2025-09-25T08:36:32.515753Z"}},"outputs":[{"name":"stdout","text":"तुम हैं तुम हैं\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}